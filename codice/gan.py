'''
Francesco Giovanelli - April 2019

Build a Generative Adversarial Network model, using existing Discriminator and Generator networks.
G and D were created to work with the N-Queens Completion problem
'''

from __future__ import print_function, division

import pandas as pd
import math
import numpy as np
import tensorflow as tf
import keras
from keras import backend as K
from keras.models import Model   
from keras.layers import * 
from keras.utils import to_categorical
from keras import optimizers
from keras.models import load_model
from keras.callbacks import TensorBoard
from keras.utils.generic_utils import Progbar
from sklearn.utils import shuffle
import logging
from collections import Counter
import sys

import my_utils
import discriminator_leaky
import generator_leaky
import generator


'''
Builds a GAN network, by loading existing Discriminator and Generator networks
'''
class GAN():

	def __init__(self):

		self.latent_dim = 64

		# Create logger to write both on console and on file
		self.logger = my_utils.create_logger_GAN()

		# Define learning rate
		lr_gan = 0.0002

		self.logger.debug("Lr_Combined: %f - Lr_Discriminator: %f" % (lr_gan, lr_gan))

		# Create optimizers for G and D
		# Trick: use SGD for D, and Adam for G
		optimizer_d = optimizers.Adam(lr=lr_gan, beta_1=0.9, beta_2=0.999, decay=1e-5)#optimizers.SGD(lr=lr_gan, decay=1e-5, momentum=0.9, nesterov=True)#
		optimizer_combined = optimizers.Adam(lr=lr_gan, beta_1=0.9, beta_2=0.999, decay=1e-5)

		# Build the discriminator
		self.discriminator = self.build_discriminator(pre_trained=True, res_blocks_num=2)

		# Re-compile discriminator: If you compile a model again, you will lose the optimizer states, but the weights will not change 
		self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer_d, metrics=['accuracy'])
		
		# Build the generator
		self.generator = self.build_generator(pre_trained=True, res_blocks_num=2, masking=True)
		
		# The generator takes partial solutions as input and generates proper assignments
		partial_solutions = Input(shape=(self.latent_dim,))
		assignments = self.generator(partial_solutions)

		# Merge partial solutions and generated assignments, to compute new generated solutions
		merging_out = keras.layers.Add()([partial_solutions, assignments])

		# Create the full Generator model (original G + Add operation)
		self.generator_full = Model(partial_solutions, merging_out)
		gen_solutions = self.generator_full(partial_solutions)

		# For the combined model we will only train the generator
		# For this to take effect, it is necessary to call compile() on the model after modifying the trainable property
		self.discriminator.trainable = False

		# The discriminator takes assignments generated by the full G as input, and determines their validity
		validity = self.discriminator(gen_solutions)
		
		# The combined model (stacked generator and discriminator)
		# Trains the generator to fool the discriminator
		self.combined = Model(partial_solutions, validity)
		self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_combined, metrics=['accuracy'])

		# Save combined model structure on file
		with open('combined_struct.txt', 'w') as model_file:
			# Pass the file handle in as a lambda function to make it callable
			self.combined.summary(print_fn=lambda x: model_file.write(x + '\n'))


	def build_generator(self, model_name="Generator", pre_trained=True, res_blocks_num=2, masking=True):

		#load existing generator from file
		print("Loading Generator network...")

		if pre_trained:
			model = load_model('generator_model_WGAN_BA_unique_50ep_leaky.h5', custom_objects={"tf": tf, "keras": keras})
		else:
			model = generator_leaky.build_generator(input_shape=(64,), res_block_num=res_blocks_num, masking_on=masking)
			
		model.name = model_name

		print("Generator network - Loaded")

		return model

	def build_discriminator(self, model_name="Discriminator", pre_trained=True, res_blocks_num=2):

		#load existing discriminator from file
		print("Loading Discriminator network...")

		if pre_trained:
			model = load_model('discriminator_model_BA_leaky.h5', custom_objects={"tf": tf, "keras": keras})
		else:
			model = discriminator_leaky.build_discriminator(input_shape=(64,), res_block_num=res_blocks_num)

		model.name = model_name

		print("Discriminator network - Loaded")

		return model


	'''
	Merges partial solutions tensor with assignments tensor
	'''
	def merging(self, tensors):

		# extract tensors
		partial_solutions = tensors[0]
		assignments = tensors[1]

		# compute tensor with only maximum values for each assignment
		max_tensor = K.max(assignments, axis=1, keepdims=True)

		# subtract from each assignment tensor its max value, so that the desired assignment become 0 and the others < 0
		assign_subtracted = keras.layers.Subtract()([assignments, max_tensor])

		# transform values that are not the desired assignments to -1s and the desired assignment to 0
		# how "sign" works : y = -1 if x < 0; 0 if x == 0; 1 if x > 0
		signed = tf.math.sign(assign_subtracted)

		# create a tensor of 1s
		ones_tensor = K.ones_like(assign_subtracted)

		# perform +1 operation, so that -1 values become 0, and 0 values become 1 (desired assignments)
		binary_mask = keras.layers.Add()([signed, ones_tensor])

		#add assignments to partial solutions
		gen_solutions = keras.layers.Add()([partial_solutions, binary_mask])

		return gen_solutions


	'''
	Performs training of the GAN
	'''
	def train(self, epochs, batch_size=64, eval_interval=10, full_eval_interval=50, train_set="B", test_set="L"):

		self.logger.info("epochs: %d | batch_size: %d" % (epochs, batch_size))

		# Load the datasets for the Discriminator network
		train_x_discrim_feas = my_utils.load_GAN_data(dataset="DS_GAN/DS.UNIQUES.FEASIBLE." + train_set + ".txt")
		train_x_discrim, train_y_discrim, test_x_discrim, test_y_discrim = my_utils.load_discriminator_data_v2(train_dataset="DS_DISCRIMINATOR/DS.UNIQUES." + train_set + ".ALL.txt", 
			test_dataset="DS_DISCRIMINATOR/DS.UNIQUES." + test_set + ".ALL.txt",
			validation=False)

		#load the 92 full solutions, for the N-Queens problem, into a dataframe
		full_solutions_df = pd.read_fwf("DS.FULL.SOLUTIONS.txt", widths=[1] * 64, header=None)

		# Load training and test set for the Generator network
		original_uniqe_trainset_gener = my_utils.load_GAN_data(dataset="DS_GENERATOR/DS.UNIQUES." + test_set + ".txt")
		original_uniqe_testset_gener = my_utils.load_GAN_data(dataset="DS_GENERATOR/DS.UNIQUES." + train_set + ".txt")

		original_trainset_gener = my_utils.load_GAN_data(dataset="DS_GENERATOR/DS.UNIQUES." + test_set + ".txt")
		original_testset_gener = my_utils.load_GAN_data(dataset="DS_GENERATOR/DS.UNIQUES." + train_set + ".txt")
		original_testset_gener_len = original_testset_gener.shape[0]
		num_samples = int(original_testset_gener_len / 2)

		# Split in two parts the DS used as the original test set for G; the first part it is used as training set, the second as test set
		original_testset_gener = shuffle(original_testset_gener)
		
		train_x_gener = original_testset_gener[:num_samples]
		test_x_gener = original_testset_gener[-num_samples:]
		'''
		# Generate noise DS for G training and G predictions
		train_x_gener, _, _ = my_utils.create_noise_ds_generator("OLD DS/DS_GENERATOR/DS.A.NEW.UNIQUES." + train_set + ".4.txt", num_single_solutions=550)
		'''

		# Adversarial ground truths
		valid = np.ones((batch_size, 1))
		valid_smooth = np.ones(batch_size) - 0.2 * np.random.uniform(0, 1, size=batch_size) #label smoothing D
		fake = np.zeros((batch_size, 1))
		fake_smooth = np.zeros(batch_size) + 0.2 * np.random.uniform(0, 1, size=batch_size) #label smoothing D

		
		# FIRST EVALUATION
		feasibility_rate, feas_ratios = my_utils.check_solutions_feasibility(self.generator, pd.DataFrame(train_x_gener[-1000:]), "DS.FULL.SOLUTIONS.txt")

		# Evaluate untrained G
		self.logger.debug("Generator - Pre-train - New train set / original test set Feas.rate: %f" % feasibility_rate)
		self.logger.debug(feas_ratios)

		# Evaluate untrained D
		train_loss, train_acc = self.discriminator.evaluate(train_x_discrim, train_y_discrim)
		self.logger.info("Discriminator - Pre-train - Train loss: %f" % train_loss)
		self.logger.info("Discriminator - Pre-train - Train accuracy: %f" % train_acc)
		

		for epoch in range(1, epochs + 1):

			print("\n### Epoch {}/{} ###".format(epoch, epochs))

			# Initialize counters for total G&D loss and G&D accuracy
			d_loss_tot = 0
			d_loss_real_tot = 0
			d_loss_fake_tot = 0
			d_acc_tot = 0
			g_loss_tot = 0
			g_acc_tot = 0

			# compute total number of batches
			num_batches = int(np.floor(train_x_gener.shape[0] / float(batch_size)))

			# set up progress bar
			progress_bar = Progbar(target=num_batches)

			# shuffle datasets
			train_x_discrim_feas = shuffle(train_x_discrim_feas)
			train_x_gener = shuffle(train_x_gener)

			illegal_assignments_counter = 0

			# iterate on every batch
			for index in range(num_batches):

				### Discriminator ###

				# Sample from discriminator dataset
				feas_solutions = train_x_discrim_feas[index * batch_size:(index + 1) * batch_size]

				# Sample from generator dataset
				gen_samples = train_x_gener[index * batch_size:(index + 1) * batch_size]

				# Use the Generator Network to produce a batch of new assignments from the sampled partial solutions
				gen_solutions = self.generator_full.predict(gen_samples)

				# Train the Discriminator on valid and fake data
				d_loss_real = self.discriminator.train_on_batch(feas_solutions, valid_smooth)
				d_loss_fake = self.discriminator.train_on_batch(gen_solutions, fake_smooth)

				d_loss_real_tot += d_loss_real[0]
				d_loss_fake_tot += d_loss_fake[0]
				
				# Compute total loss
				d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
				d_loss_tot += d_loss[0]
				d_acc_tot += d_loss[1]


				### Generator ###

				# Sample randomly from generator dataset
				gen_samples = train_x_gener[index * batch_size:(index + 1) * batch_size]

				# Train the generator (to have the discriminator label samples as valid)
				g_loss, g_acc = self.combined.train_on_batch(gen_samples, valid)

				g_loss_tot += g_loss
				g_acc_tot += g_acc

				progress_bar.update(index + 1)


			# Print loss and accuracy
			d_loss_avg = d_loss_tot/num_batches
			d_loss_real_avg = d_loss_real_tot/num_batches
			d_loss_fake_avg = d_loss_fake_tot/num_batches
			d_acc_avg = d_acc_tot/num_batches
			g_loss_avg = g_loss_tot/num_batches
			g_acc_avg = g_acc_tot/num_batches
			self.logger.info("---------------------------------------------------")
			self.logger.info("%d [D loss: %f (real: %f, fake: %f), acc.: %.2f] [G loss: %f, acc.: %.2f]" % 
				(epoch, d_loss_avg, d_loss_real_avg, d_loss_fake_avg, d_acc_avg, g_loss_avg, g_acc_avg))
			self.logger.info("---------------------------------------------------")

			# If at interval, compute the feasibility of generator's outputs on the full training & test sets, and eval discriminator on train & test sets
			if epoch % full_eval_interval == 0:

				# check feasibility of assignments produced by the generator
				global_feas_rate, feas_ratios = my_utils.check_solutions_feasibility(self.generator, pd.DataFrame(original_uniqe_testset_gener), "DS.FULL.SOLUTIONS.txt")
				
				# Print only on file
				self.logger.debug("Generator - Train set GAN - Feas.rate: %f" % global_feas_rate)
				self.logger.debug(feas_ratios)

				# check feasibility of assignments produced by the generator
				global_feas_rate, feas_ratios = my_utils.check_solutions_feasibility(self.generator, pd.DataFrame(original_uniqe_trainset_gener[-3000:]), "DS.FULL.SOLUTIONS.txt")
				
				# Print only on file
				self.logger.debug("Generator - Original Train set / test set GAN - Feas.rate: %f" % global_feas_rate)
				self.logger.debug(feas_ratios)

				# Evaluate discriminator on training set
				train_loss, train_acc = self.discriminator.evaluate(train_x_discrim, train_y_discrim)
				self.logger.info("Discriminator - Train set GAN loss: %f" % train_loss)
				self.logger.info("Discriminator - Train set GAN accuracy: %f" % train_acc)

				# Evaluate discriminator on test set
				test_loss, test_acc = self.discriminator.evaluate(test_x_discrim, test_y_discrim)
				self.logger.info("Discriminator - Original Train set GAN loss: %f" % test_loss)
				self.logger.info("Discriminator - Original Train set GAN accuracy: %f" % test_acc)

			# If at interval, compute the feasibility of G on a small test set and evaluate D on training set
			elif epoch % eval_interval == 0:

				# check feasibility of assignments produced by the generator
				feasibility_rate, feas_ratios = my_utils.check_solutions_feasibility(self.generator, pd.DataFrame(train_x_gener[-1000:]), "DS.FULL.SOLUTIONS.txt")
				
				# Print only on file
				self.logger.debug("Generator - Train set GAN (original test set) - Feas.rate: %f" % feasibility_rate)
				self.logger.debug(feas_ratios)
				
				# Evaluate discriminator on training set
				train_loss, train_acc = self.discriminator.evaluate(train_x_discrim, train_y_discrim)
				self.logger.info("Discriminator - Train set GAN loss: %f" % train_loss)
				self.logger.info("Discriminator - Train set GAN accuracy: %f" % train_acc)



if __name__ == '__main__':
	gan = GAN()
	gan.train(epochs=100, batch_size=64, eval_interval=10, full_eval_interval=50, train_set="A", test_set="B")